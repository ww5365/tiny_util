
# 排序模型

cart -> gdbt -> Gbrank  landmart  -> deepFM

xgboost

lightGBM

catboost


## 排序特征

### 文本相关特征

CQR

OFFSET

edit distance

lcs

euclidean distance

cosine similarity

jaccard similarity

Pearson

tf-idf

bm25

### 点击特征

CTR

用户需求分布



### 深度特征

NN： 



## 排序模型

### lambdaMart

重点参考下这篇博文：

http://www.yinkuiwang.cn/2021/12/11/LambdaMart%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/

代码：
https://github.com/jiangnanboy/learning_to_rank

https://everdark.github.io/k9/notebooks/ml/learning_to_rank/learning_to_rank.html

一句话点评：

将评价指标,比如NDCG引入到损失函数， 不可导的指标函数构构造可到的实现； 通过lamda来控制下一轮迭代的大小和方向。

牛顿法进行损失函数最优解的求解

1. https://blog.csdn.net/wuzhongqiang/article/details/110521519





### 损失函数类型 

支持gbrank loss、lambdamart dcg loss、lambdamart ndcg loss及regression loss类型。[出自链接](https://www.alibabacloud.com/help/zh/machine-learning-platform-for-ai/latest/gbdt-regression)


### catboost - yetirankpairwise

https://catboost.ai/en/docs/concepts/loss-functions-ranking

论文1：
Winning The Transfer Learning Track of Yahoo!’s Learning
To Rank Challenge with YetiRank ：  http://proceedings.mlr.press/v14/gulin11a/gulin11a.pdf

论文2：
https://arxiv.org/pdf/2204.01500.pdf  ： Which Tricks are Important for Learning to Rank?

博文：
https://neptune.ai/blog/when-to-choose-catboost-over-xgboost-or-lightgbm

CatBoost has a ranking mode – CatBoostRanking just like XGBoost ranker and LightGBM ranker, however, it provides many more powerful variations than XGBoost and LightGBM. The variations are:
Ranking (YetiRank, YetiRankPairwise)
Pairwise (PairLogit, PairLogitPairwise)
Ranking + Classification (QueryCrossEntropy)
Ranking + Regression (QueryRMSE)
Select top 1 candidate (QuerySoftMax)

ranking 和 pairwise 还有区分？
yetirank  不知道什么东西？






## reference

算法知识点——（5）集成算法—GBDT详解  : https://blog.csdn.net/Lynqwest/article/details/101540952

GBDT C++ 实现代码：https://github.com/yarny/gbdt  
    GBDT implements various pointwise, pairwise, listingwis loss functions including mse, logloss, huberized hinge loss, pairwise logloss, GBRank and LambdaMart. It supports easily addition of your own custom loss functions.

GBDT：
https://blog.csdn.net/twlve/article/details/124179409

https://zhuanlan.zhihu.com/p/280222403 :  这篇博文，有CART树的python实现，GBDT的实例图但没有看懂那两张图，参考下面readme中博文来理解.

https://github.com/Freemanzxp/GBDT_Simple_Tutorial :  实例图展示


XGBoost:
https://zhuanlan.zhihu.com/p/162001079  :  还是以这篇博文，构成xgboost整体的体系


工业界实际应用：xgboost lightgbm  catboost




Learning to Rank算法学习之GBRank: https://www.biaodianfu.com/gbrank.html

GBrank 代码： https://github.com/szdr/my-gbrank  

GBRank的问题列表 ： https://daimajiaoliu.com/daima/47dd2271c9003fc

gbrank排序为什么会造成预测值为负的原因 : https://blog.csdn.net/a1066196847/article/details/83382708

[搜索排序算法] (https://octopuscoder.github.io/2020/01/19/%E6%90%9C%E7%B4%A2%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/)  
    LambdaRank，基于xgboost实现，参考：https://github.com/dmlc/xgboost/tree/master/demo/rank
    LambdaMART，基于LightGBM实现，参考：https://github.com/jiangnanboy/learning_to_rank
    Wide&Deep基于TensorFlow实现，参考：https://github.com/tensorflow/ranking

利用lightgbm做learning to rank 排序 : https://github.com/jiangnanboy/learning_to_rank 





## tanxin-推荐- Ranking

>> week7 经典ranking

### 经典ranking - CTR


工程经验更多： 模型工作量很小，主要是做数据和特征

排序的性能问题需要关注：性能提升的思路： 工程团队支撑， 加机器， 异步调用 


排序中常用特征：  千万维度 甚至 亿级特征

- user 侧特征
- item 侧特征
- 上下文特征  ：  机器环境，地理信息...
- 交叉特征 ： ?
- 匹配特征 ： ?


LR：

$$y = F(x) = sigmod(W^T*X)$$

优势：

- 轻量级
- 端到端


算法工程师：

模型的理解能力： 加某个特征就是有效？ 哪个交叉特征有用？ 效果
业务的能力：  什么东西work？
工程优化能力： 真正给企业带来流量 效益


LR -> deepFM -> DIN :  整体1%  相对10-15%

常用排序模型：
LR FM gbdt gbdt+lr wide&deep  deepFM


协调和管理能力  +  战略能力  ： cto

GBDT:


集成模型：
bagging
boosting


面试可能的问题？

训练好了一个gbdt模型，怎么预测体重：140  ?


gbdt 不擅长 高纬稀疏的离散特征, 为什么？思考 LR 更擅长


GBDT + LR :

facebook

两个模型更新的频率是一致的吗？ gbdt 更新的慢， lr
高纬稀疏特征： 离散特征直接进入lr，连续特征过gbdt离散化后，再送入lr


Xgboost:


二阶泰勒展开


自变量是什么？



















