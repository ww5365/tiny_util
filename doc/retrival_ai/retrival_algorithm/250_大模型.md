# 大模型


## 贪心大模型微调实战
### 第一节 

#### 1.4 transformer应用

参考资料：
* https://jalammar.github.io/illustrated-transformer/
* https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452


### 第三节

### 课程相关资料

####相关论文：
https://arxiv.org/pdf/2106.09685.pdf
https://arxiv.org/pdf/2308.10792.pdf   # 这篇文章中有开放的instruction data
https://arxiv.org/pdf/1907.11692.pdf
https://arxiv.org/pdf/2212.10560.pdf
https://arxiv.org/pdf/2402.06196.pdf

#### BERT介绍博客
https://jalammar.github.io/illustrated-bert/
#### Vicuna介绍
https://lmsys.org/blog/2023-03-30-vicuna/
#### Lora的从零实现
https://medium.com/towards-data-science/implementing-lora-from-scratch-20f838b046f1

#### alpaca-lora finetune
https://github.com/tloen/alpaca-lora/tree/main
#### LoRA从零实现配套代码
https://github.com/Montinger/Transformer-Workbench/tree/main/LoRA-from-scratch
#### self-instruct论文配套代码
https://github.com/yizhongw/self-instruct/tree/main
#### Lora paper的实现代码
https://github.com/microsoft/LoRA/blob/main/loralib/utils.py
#### PEFT的library代码
https://github.com/huggingface/peft
#### Roberta的源码
https://huggingface.co/transformers/v4.3.3/_modules/transformers/models/roberta/modeling_roberta.html



