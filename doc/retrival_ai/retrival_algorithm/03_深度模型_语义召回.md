# 

#


MLP
CNN
LSTM
Transformer
BERT


DSSM
ANN 
HNSW  
faiss



## transformer专题

核心材料：
《This post is all you need（上卷）——层层剥开Transformer V1.3.2》

#### 材料学习中的疑问


#### nn.embedding 理解，即NLP使用的NN网络的输入

https://blog.csdn.net/raelum/article/details/125462028 ： 深入理解torch中的nn.Embedding
   1) torch.nn.Embedding 和  nn.Linear  异同：
      同：内容都有weight权重矩阵
      异：nn.Embdding输入是离散id的索引list ，weight矩阵类似词表，使用索引值查找weight，产生初始词向量。
          nn.Linear 输入是one-hot向量，通过与weight的矩阵运算，获取输出神经元的值
https://www.cnblogs.com/ryukirin/p/14612795.html
   主要理解：nn.Embedding的输入参数，及入参shape，输出的结果shape


#### 多head原因？ 多个head头，怎么实现每个头注意不同的感受野？

多个不同的子空间
W_Q， W_K， W_V 是多个head的，初始化是多head的维度，进行的初始化： self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))





#### add & Normal
transformer中的nn.LayerNorm(dim)针对的是token的词向量维度

#### decoder中en-de multihead attention中的KV 输入获取








#### 

深入理解Transformer及其源码 : https://www.cnblogs.com/zingp/p/11696111.html


What does PyTorch Embedding do? ： https://www.quora.com/What-does-PyTorch-Embedding-do
终于碰上torch.nn.Embedding ：https://blog.csdn.net/qq_41329791/article/details/109182823


### 国外的文章，写的真清楚啊
https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34

https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853



## 语义相关项目

   
   D:\workfiles\20220906_大搜搜索\20220119_deepmatch
   语义召回模型学习过程.md
   
   D:\workfiles\20220906_大搜搜索\20220920_精排资料\deep
   
   
   http://wenote.xxx.com/wapp/recent/75283210-1f2d-11eb-9ec0-5b0de86f74be?docGuid=8ad22250-46dc-11ed-94a0-4b397f3a3391
   
   语义召回对用MTP上东东？
   
   语义相关原理网上参考： https://github.com/ww5365/tiny_util/blob/master/doc/retrival_ai/retrival_algorithm/03_%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B_%E8%AF%AD%E4%B9%89%E5%8F%AC%E5%9B%9E.md
   
   
   D:\workspace\my_codehub\myproject\bd_sparklesearch_semantic ：深度语义代码
   
   http://7.185.31.49:3000/9rFac158QVWqTdoVkZ3cpg?view  ： 实习生的产出

## DSSM

微软DSSM模型（Deep Structured Semantic Models）

1. 参考1.重点能说明问题  :  [推荐系统（十七）双塔模型：微软DSSM模型](https://blog.csdn.net/u012328159/article/details/123782735 )

* DSSM 离线训练，正样本是点击的item，负样本为王？ 负样本怎么选择？

* 在线应用时，训练的模型，离线索引，是怎么生效的？
  
  query侧的模型 和  doc侧的模型可以是不同模型吗？

![img](../img/20220515-163841-dssm.jpg)

  

2. 参考2. 最基本的DSSM原理的介绍 ：  [DSSM](https://paddlepedia.readthedocs.io/en/latest/tutorials/recommendation_system/dssm.html#id2)


3. 参考3. [YouTube采样修正的双塔模型论文精读](https://mp.weixin.qq.com/s/us4qGD3LDgLmPy2m-qq-iw#at)

《Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations》

知道了问题： 

1、 双塔召回 和 (ANN) faiss库 是什么关系？

向量模型训练完后，在线应用时，query 侧产利用模型，实时产出语义向量 qVector +  doc侧利用模型离线构建语义向量索引，docVectors； 利用向量搜索的引擎，比如faiss，使用qVector到docVectors中进行向量索引的搜索。



4. 参考4. 双塔DNN召回模型/DSSM算法(https://www.cnblogs.com/charlesblc/p/15932373.html)


5. 参考5：[语义向量召回之ANN检索](http://xtf615.com/2020/08/01/EBR/) 

   基于Embedding-based Retrieval in Facebook Search 这篇论文，深入探讨并get到如下的思想：

   * 语义向量搜索系统的系统框架 ：  关注query侧模型  doc侧模型所处位置
   * 重点讨论了：向量如何进行 向量量化 ? PQ量化   目的是：优化向量存储，向量搜索效率  向量搜索的时空复杂度

     调参的经验：
     多尝试使用乘积量化：包括原生PQ，OPQ，PQ with PCA等。能够显著降低索引存储空间存储复杂度、查询时间复杂度。其中，pq_bytes，即残差量化codes的大小，很重要。决定了残差量化分块聚类中心的个数，个数越大越精确，比如m=8,k*=256时，pq_byte=1byte x 8= 8 byte。paper中建议采用d/4，d是向量的维度数，假设64维度，则d/4=16，是默认值的2倍

6. 参考6: [负样本为王：评Facebook的向量化召回算法](https://zhuanlan.zhihu.com/p/165064102)

   基于Embedding-based Retrieval in Facebook Search 主要讲负样本怎么处理？
   
   召回： 主要是做负样本

   负样本： 随机采样  hard negtive mining samples 基于上一个召回模型，取100~500之间的样本，这种看起来像，但又不是正样本的，增加学习的难度，学到细微的特征不同点

7. 参考7：[语义索引（向量检索）的几类经典方法](https://zhuanlan.zhihu.com/p/161467314)

    这篇博文，也是基于Embedding-based Retrieval in Facebook Search

    这篇论文的一个梳理


8. 参考8： KDD'21 | 淘宝搜索中语义向量检索技术 : https://zhuanlan.zhihu.com/p/409390150

   这篇文章是facebook这篇文章的延伸，关注下模型的网络结构，难度更大些。


   https://www.6aiq.com/article/1617063677499  ： 深度语义模型 BERT 在 58 同城搜索的实践

   关注其中的：双塔式BERT 主要是向量部分使用bert获得

   https://www.6aiq.com/article/1605651191093 : 阿里文娱深度语义搜索相关性探索

   这篇文章介绍了，非对称双塔模型（在线部署），上篇58同城的文章中也有提到。 对于双塔模型，如果用bert来表示文本语义向量， 他的这个双塔模型，特别是向量表示模型，选用bert，是可以参考这种做法的。

   poly-encoder 模型  :  双塔 + bert + attention
   
   论文中的工程实践：

   打标： 用交互型的BERT（指标最好，但只能离线使用）作为teacher。在以上两个数据上进行打标，得到BERT soft label。
   
   蒸馏： 用无标签的数据蒸馏。并不是用BERT直接蒸馏目标模型，而是用它去蒸馏对称的双塔BERT，也就是说Query侧和Doc侧都是12层的BERT。
   
   用target set 进行finetune： 获得了中间模态的对称双塔BERT。
   
   固定权重和蒸馏



















## 参考

1. 推荐系统（十七）双塔模型：微软DSSM模型（Deep Structured Semantic Models） ： https://blog.csdn.net/u012328159/article/details/123782735 

2. DSSM: https://paddlepedia.readthedocs.io/en/latest/tutorials/recommendation_system/dssm.html#id2

3. YouTube采样修正的双塔模型论文精读 : https://mp.weixin.qq.com/s/us4qGD3LDgLmPy2m-qq-iw#at

4. 双塔DNN召回模型/DSSM算法  :  https://www.cnblogs.com/charlesblc/p/15932373.html

5. 语义向量召回之ANN检索： http://xtf615.com/2020/08/01/EBR/
6.  [负样本为王：评Facebook的向量化召回算法](https://zhuanlan.zhihu.com/p/165064102)
