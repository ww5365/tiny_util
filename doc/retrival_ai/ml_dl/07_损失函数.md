# 损失函数


## CrossEntropyLoss

交叉熵损失: 用来描述两个概率分布p,q的距离，交叉熵值越小，说明两个概率分布越接近，距离越近。主要两种实现方式：


* 输出层对应sigmod输出
$$CE(y,\widehat{y}) = -{\frac{1}{N}}\sum_{i=0}^N({y_ilog({\widehat{y_i}}) + (1-y_i)log(1-\widehat{y_i})})$$

说明：一般二分类问题中使用

* 输出层对应softmax输出

$$-{\frac{1}{N}}\sum_{i=0}^N(y_ilog(\widehat{y_i}))$$

说明：
1) 多分类问题可用
2) pytorch中提供nn.CrossEntropyLoss()函数实现( <==> nn.LogSoftMax() 和 nn.NLLLoss())
   * 输出层的结果先进行softmax，将值划到[0,1]
   * 再进行取log的运算，将值划到[-infinity, 0] ,实际以上两步是：nn.LogSoftMax() 操作
   * 再将结果取负值，求均值， 实际是：nn.NLLLoss()

总结：两种形式的交叉熵损失主要对应了两种不同的输出层


参考：
1. [交叉熵损失函数 CrossEntropyLoss() 详解](https://blog.csdn.net/weixin_44211968/article/details/123906631)
2. [两种交叉熵损失函数的异同](https://blog.csdn.net/u012436149/article/details/69660214)



## MultipleNegativesRankingLoss

![image](https://github.com/ww5365/tiny_util/assets/15375027/82e5e1af-00e2-4ba9-b8c4-227880b8372e)

说明：
1) 这个损失函数，针对只有正样本，没有负样本的场景，还是很好用的。all-MiniLM-L6-v2模型[详见]()就是使用的这损失函数。
2) 通过最后的BatchLoss可以看出，如果想要训练的整体损失最小，需要把神经网络的输出s(x,y) 正例:S(query, positive_sample) 得分，搞的大大的； 负例：S(query, neg_positive_sample)得分，搞的小小的，符合设计预期
3) Loss要么直接计算，比如CE， 要么对比计算，让相似的更近，不想似的更远
4) 这个损失函数有Sentence-transformer直接实现，可以使用，参考：https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss


参考：
1. [表征学习Loss Function总结
](https://zhuanlan.zhihu.com/p/456909073)
2. [SimCSE：句子嵌入的简单对比学习 && 理解对比学习损失函数及温度系数](https://blog.csdn.net/u011239443/article/details/120322145) : 这个里面对比学习的损失函数，实际也是相同的。无监督的对比学习的正样本构造方法，很巧妙
3. [Multiple Negatives Ranking Loss for Sentence Embeddings](https://www.youtube.com/watch?v=b_2v9Hpfnbw) : 这个视频中有个简单的数据示例。但没有想清楚，n*m 矩阵，是什么粒度的相关性矩阵？





