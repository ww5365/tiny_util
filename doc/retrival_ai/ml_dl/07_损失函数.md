# 损失函数


## CrossEntropyLoss

交叉熵损失: 用来描述两个概率分布p,q的距离，交叉熵值越小，说明两个概率分布越接近，距离越近。主要两种实现方式：


* 输出层对应sigmod输出
$$CE(y,\widehat{y}) = -{\frac{1}{N}}\sum_{i=0}^N({y_ilog({\widehat{y_i}}) + (1-y_i)log(1-\widehat{y_i})})$$

说明：一般二分类问题中使用

* 输出层对应softmax输出

$$-{\frac{1}{N}}\sum_{i=0}^N(y_ilog(\widehat{y_i})$$

说明：
1) 多分类问题可用
2) pytorch中提供nn.CrossEntropyLoss()函数实现( <==> nn.LogSoftMax() 和 nn.NLLLoss())
   * 输出层的结果先进行softmax，将值划到[0,1]
   * 再进行取log的运算，将值划到[-infinity, 0] ,实际以上两步是：nn.LogSoftMax() 操作
   * 再将结果取负值，求均值， 实际是：nn.NLLLoss()

总结：两种形式的交叉熵损失主要对应了两种不同的输出层


参考：
https://blog.csdn.net/weixin_44211968/article/details/123906631
https://blog.csdn.net/u012436149/article/details/69660214



## MultipleNegativesRankingLoss



