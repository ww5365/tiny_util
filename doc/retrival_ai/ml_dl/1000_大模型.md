# 大模型

## 2024.4.24

llma3公开课  ：

llam3 的introduction的url：
Introducing Meta Llama 3: The most capable openly available LLM to date: https://ai.meta.com/blog/meta-llama-3


oolama: 没有gpu也可以跑
github.com/ollama/ollama


tiktokenizer:
llama3 源码使用tokenizer：

tokenzier示例：
https://tiktokenizer.vercel.app/?

bpe算法：

https://github.com/openai/tiktoken/blob/main/tiktoken/_educational.py :  bpe_train  可以参考


Key cache： 工程化优化性能

localhost:8888/notebooks/GroupedQueryAttention.ipyub  

空间大小： l * b * n * h * s  * 2 * 2

group-query attentin 优化n  k v


RoPE : Rotary Positional Embedding

原来使用的是：absolute positional embedding  : 每个位置的向量已设计好了








