mk'
l7f
## 第8周


### 知识图谱与LLM

KGE： 实体+关系 进行向量化

**- TransE ：  这个需要理解？**



代码示例：26'43''

- rec
- role-based-generate-data ： 
- SFT推荐模型微调
- 基于知识图谱的问答系统


医疗知识图谱：

- 综述： clinical insights : A comprehensive review of lm in medicine    目的：?  刚开始大概知道别人怎么做

- Huatuo ： 华驼  intruction-tuning lm with chinese medical knowledge
  怎么利用知识图谱结构化数据，构建指令微调数据？knowledge-tuning
  
  1). 利用kg，构造instruction, 根据input， 利用llm获取得到output。得到QA的对。 

- 上面是self-instruct  self-qa 生成微调数据
  
- role-based-generate-data ： llm角色扮演生成数据

  https://github.com/shibing624/MedicalGPT/tree/main/role_play_data
  
- dify ： 开源，轻量的工作流编排工具，agent？ translation agent 
- chatlaw : 全面多样的法律数据集， 经过人工微调，生成高质量QA，kg，agent的数据集合

  https://chatlaw.cloud/
  https://github.com/PKU-YuanGroup/ChatLaw
  
- 知识图谱来做QA，类似RAG

  https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo/
  https://python.langchain.com/docs/tutorials/graph/
  
- 利用llm构建kg ： llama_index(knowledgeGraphIndex) , langchain(LLMGraphTransformer) : 1:09:48 参考如下：
  https://python.langchain.com/docs/how_to/graph_constructing/#llm-graph-transformer
  https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo/  
  



#### 本节其它资料
- 陈华钧教授公开课：https://www.icourse163.org/course/ZJU-1464119172?from=searchPage&outVendor=zw_mooc_pcssjg_
- openkg 中文开放知识图谱 ： http://openkg.cn/
- openKE 清华的图谱的embeding： https://github.com/thunlp/OpenKE   面壁智能 MiniCPM  端侧智能


### 图神经网络和LLM

- 初步介绍图神经网络： 参考：Qcon 《图机器学习在征信解读上的创新应用》- 李祥  度小满  [链接](https://www.infoq.cn/article/wsg1e5yyvockjzj1vcqu)
- cora ： 数据集  图神经网络，示例使用的数据集合
  [原始数据链接](https://www.cs.umd.edu/~sen/lbc-proj/LBC.html)
  [参考链接](https://www.cnblogs.com/Catherinezhilin/p/16288734.html)
  
- 典型的图神经网络模型

  - GCN ： semi-supervised classification with graph convolutional networks   19:14

  **26:40  这个有GCN的代码实现的演示**

  - textGCN：  Graph Convolutional network for text classification 2019

  如何将text装换成图表示： 文章名称作为起始节点， word nodes， word和word之间的边如何连接？ PMI(i,j)  互信息

  - GAT: graph attention networks

    **没懂**

  - graphSAGE ： 

    用了可视化的图来展示了graphSAGE示例

  - graph pre-training ：

    contrastive
    generative
 
    simCSE  ： 陈丹琦

  - graph + LLM

    GNN和LLM都有各自更擅长的事情，GNN更擅长结构化的东西，LLM更擅长从非结构化的文本中理解出结构化的信息;
    利用知识图谱进行推理增强
 
    LLM和graph的结合思路：

    ![微信图片_20241105163222](https://github.com/user-attachments/assets/a7b9cfa3-6a3f-459c-a1e8-939b923de488)
    
      - llm as Enhancer  ： llm提供特征，输给GNN，两种方式：explanation-based/embedding-based。LLM给GNN提供支撑。
      - llm as predictor :  flatten-based/GNN-based : 这个思路是我们关注的KG增强LLM的思路。现在来看最有可能的是，flatten-based的思路，限制是：llm token长度，只能使用graph中几步内的neighbors信息。
      - GNN-LLM alignment： 对齐？  多模态对齐，给了四种方式  
        








