# 贪心大模型微调实战
## 第一节 

### 1.4 transformer应用

参考资料：
* https://jalammar.github.io/illustrated-transformer/
* https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452


## 第三节

## 课程相关资料

### 相关论文：
https://arxiv.org/pdf/2106.09685.pdf
https://arxiv.org/pdf/2308.10792.pdf   // 这篇文章中有开放的instruction data
https://arxiv.org/pdf/1907.11692.pdf
https://arxiv.org/pdf/2212.10560.pdfhttps://arxiv.org/pdf/2402.06196.pdf

### BERT介绍博客
https://jalammar.github.io/illustrated-bert/
### Vicuna介绍
https://lmsys.org/blog/2023-03-30-vicuna/
### Lora的从零实现
https://medium.com/towards-data-science/implementing-lora-from-scratch-20f838b046f1

### alpaca-lora finetune
https://github.com/tloen/alpaca-lora/tree/main
### LoRA从零实现配套代码
https://github.com/Montinger/Transformer-Workbench/tree/main/LoRA-from-scratch
### self-instruct论文配套代码
https://github.com/yizhongw/self-instruct/tree/main
### Lora paper的实现代码
https://github.com/microsoft/LoRA/blob/main/loralib/utils.py
### PEFT的library代码
https://github.com/huggingface/peft
### Roberta的源码
https://huggingface.co/transformers/v4.3.3/_modules/transformers/models/roberta/modeling_roberta.html



## 2024.4.24

llma3公开课  ：

llam3 的introduction的url：
Introducing Meta Llama 3: The most capable openly available LLM to date: https://ai.meta.com/blog/meta-llama-3


oolama: 没有gpu也可以跑
github.com/ollama/ollama


tiktokenizer:
llama3 源码使用tokenizer：

tokenzier示例：
https://tiktokenizer.vercel.app/?

bpe算法：

https://github.com/openai/tiktoken/blob/main/tiktoken/_educational.py :  bpe_train  可以参考


Key cache： 工程化优化性能

localhost:8888/notebooks/GroupedQueryAttention.ipyub  

空间大小： l * b * n * h * s  * 2 * 2

group-query attentin 优化n  k v


RoPE : Rotary Positional Embedding

原来使用的是：absolute positional embedding  : 每个位置的向量已设计好了


## 第四课

大模型训练，llma2, 常用的公开的数据集：CC  common 

构造样本： 有个图展示了自动化构造样本的全过程，  

instruction
instance： input output








